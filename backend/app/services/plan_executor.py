"""
Plan Executor Service

Executes dynamic execution plans generated by the Planner Agent.
Handles dependency resolution, parallel execution, and STATE-BASED context management.
"""

import asyncio
from app.core.centralized_logger import get_logger
import sys
import os
from typing import Dict, Any, List, Tuple, Set
from collections import defaultdict

# Add MCP server to path for tool contract imports
backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
mcp_server_path = os.path.join(backend_dir, 'mcp_server')
if mcp_server_path not in sys.path:
    sys.path.insert(0, mcp_server_path)

from tool_contracts import get_tool_contracts_dict  # noqa: E402

logger = get_logger(__name__)

def _load_tool_registry() -> Dict[str, Any]:
    """
    Dynamically load all tools from tool contracts.
    Auto-discovers tools by importing modules based on contract names.

    Returns:
        Dictionary mapping tool names to their functions
    """
    import importlib

    registry = {}
    contracts = get_tool_contracts_dict()

    for tool_name in contracts.keys():
        try:
            # Import the tool module dynamically
            # e.g., "product_search" -> import mcp_server.tools.product_search
            module = importlib.import_module(f"mcp_server.tools.{tool_name}")

            # Get the function with the same name as the module
            # e.g., product_search module has product_search() function
            tool_func = getattr(module, tool_name)

            registry[tool_name] = tool_func
            logger.debug(f"‚úì Loaded tool: {tool_name}")

        except (ImportError, AttributeError) as e:
            logger.warning(f"‚ö†Ô∏è  Failed to load tool {tool_name}: {e}")
            continue

    logger.info(f"üì¶ Loaded {len(registry)} tools dynamically")
    return registry

# Tool registry for direct execution (loaded once at module import)
TOOL_REGISTRY = _load_tool_registry()

# Global list for streaming tool citations in real-time (thread-safe with GIL)
_tool_citation_callbacks: List = []


def register_tool_citation_callback(callback):
    """Register a callback function to be called when tool citations are emitted"""
    _tool_citation_callbacks.append(callback)


def clear_tool_citation_callbacks():
    """Clear all registered callbacks"""
    _tool_citation_callbacks.clear()


def get_tool_citation_callbacks():
    """Get all registered callbacks"""
    return _tool_citation_callbacks


class PlanExecutor:
    """
    Executes plans with support for:
    - Dependency resolution (topological sort)
    - Parallel execution within steps
    - Context management (args_from resolution)
    - Error handling and graceful degradation
    """

    def __init__(self):
        self.context: Dict[str, Any] = {}
        self.state: Dict[str, Any] = {}
        self.tool_citations: List[Dict[str, str]] = []  # Track tool citation messages

    async def _call_tool_direct(self, tool_name: str, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Call a tool directly (in-process) without MCP subprocess.

        Args:
            tool_name: Name of the tool to call
            state: Current state dictionary

        Returns:
            Tool result dictionary
        """
        if tool_name not in TOOL_REGISTRY:
            available = ", ".join(TOOL_REGISTRY.keys())
            raise ValueError(f"Tool '{tool_name}' not found. Available: {available}")

        try:
            tool_func = TOOL_REGISTRY[tool_name]

            # Log tool call
            logger.info(f"üîß Calling tool directly: {tool_name}")

            # Execute tool directly (no subprocess, no MCP protocol)
            result = await tool_func(state)

            logger.info(f"‚úÖ Tool {tool_name} completed (direct call)")
            return result

        except Exception as e:
            logger.error(f"‚ùå Tool {tool_name} direct call failed: {e}", exc_info=True)
            return {"error": str(e), "success": False}

    async def execute(self, plan: Dict[str, Any], state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute a plan and return results.

        Args:
            plan: Execution plan from Planner Agent
            state: Current graph state

        Returns:
            Dict with execution results and updated state fields
        """
        # Validate plan has steps
        if not plan or "steps" not in plan or not plan["steps"]:
            logger.error(f"‚ùå Plan Executor received invalid or empty plan: {plan}")
            return {
                "assistant_text": "I encountered an error creating an execution plan. Please try rephrasing your request.",
                "status": "error",
                "error": "Invalid or empty execution plan"
            }

        logger.info(f"üöÄ Plan Executor starting with {len(plan['steps'])} steps")

        # Reset context for this execution
        self.context = {}
        self.state = state  # Store state for access in arg resolution
        self.tool_citations = []  # Reset tool citations

        # DEBUG: Log slots being passed to Plan Executor
        slots = state.get("slots", {})
        logger.info(f"üîç DEBUG: Plan Executor received slots: {slots}")

        # Sort steps by dependencies (topological sort)
        sorted_steps = self._topological_sort(plan["steps"])

        logger.info(f"üìä Execution order: {[step['id'] for step in sorted_steps]}")

        # Execute steps in order
        for step_index, step in enumerate(sorted_steps, 1):
            step_id = step["id"]
            logger.info(f"‚ñ∂Ô∏è  Step {step_index}/{len(sorted_steps)}: {step_id} - {step.get('description', '')}")

            try:
                # Execute this step (direct tool calls, no MCP client needed)
                await self._execute_step(step)

                logger.info(f"‚úÖ Step {step_id} completed")

            except Exception as e:
                logger.error(f"‚ùå Step {step_id} failed: {e}", exc_info=True)

                # Check if this is a critical failure
                if self._is_critical_step(step):
                    logger.error(f"Critical step {step_id} failed, aborting execution")
                    raise
                else:
                    logger.warning(f"Non-critical step {step_id} failed, continuing...")
                    # Store error in context
                    self.context[f"{step_id}.error"] = str(e)

        # Extract final results from context
        results = self._extract_results()

        logger.info(f"üéâ Plan execution completed")

        return results

    async def _execute_step(self, step: Dict[str, Any]) -> None:
        """
        Execute a single step using STATE-BASED execution with DIRECT tool calls.
        Tools now receive arguments from shared state based on their contracts.
        Tools are called directly in-process (no MCP subprocess overhead).

        Args:
            step: Step definition with list of tool names
        """
        step_id = step["id"]
        tool_names = step["tools"]  # Now a list of strings, not dicts
        parallel = step.get("parallel", False)

        # Get tool contracts
        contracts = get_tool_contracts_dict()

        if parallel and len(tool_names) > 1:
            # Execute tools in parallel
            logger.info(f"  ‚ö° Running {len(tool_names)} tools in parallel")

            # Emit citation messages for all tools before starting
            for tool_name in tool_names:
                contract = contracts.get(tool_name)
                if contract and contract.get("citation_message"):
                    await self._emit_tool_citation(tool_name, contract["citation_message"])

            tasks = []
            for tool_name in tool_names:
                # Direct tool call - pass serializable state
                # Note: Callbacks are NOT in state - they're inherited from LangGraph context
                serializable_state = self._make_serializable(self.state)
                tasks.append(self._call_tool_direct(tool_name, serializable_state))

            # Gather results
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Store results back into state
            for tool_name, result in zip(tool_names, results):
                if isinstance(result, Exception):
                    logger.error(f"  ‚ùå Tool {tool_name} failed: {result}")
                    self.context[f"{step_id}.{tool_name}"] = {"error": str(result), "success": False}
                else:
                    logger.info(f"  ‚úì Tool {tool_name} completed")
                    self.context[f"{step_id}.{tool_name}"] = result
                    # Write tool outputs back to state
                    self._write_tool_outputs_to_state(tool_name, result, contracts.get(tool_name))

        else:
            # Execute tools sequentially
            for tool_name in tool_names:
                logger.info(f"  üîß Calling tool: {tool_name}")

                # Emit citation message before calling tool
                contract = contracts.get(tool_name)
                if contract and contract.get("citation_message"):
                    await self._emit_tool_citation(tool_name, contract["citation_message"])

                try:
                    # Direct tool call - pass serializable state (no MCP subprocess)
                    # Note: Callbacks are NOT in state - they're inherited from LangGraph context
                    serializable_state = self._make_serializable(self.state)
                    result = await self._call_tool_direct(tool_name, serializable_state)

                    self.context[f"{step_id}.{tool_name}"] = result
                    logger.info(f"  ‚úì Tool {tool_name} completed")

                    # Write tool outputs back to state
                    self._write_tool_outputs_to_state(tool_name, result, contracts.get(tool_name))

                except Exception as e:
                    logger.error(f"  ‚ùå Tool {tool_name} failed: {e}")
                    self.context[f"{step_id}.{tool_name}"] = {"error": str(e), "success": False}
                    raise

    async def _emit_tool_citation(self, tool_name: str, citation_message: str) -> None:
        """
        Emit a citation message for a tool that's about to run.
        This is stored for later inclusion in the results AND calls registered callbacks for streaming.
        """
        citation = {
            "tool": tool_name,
            "message": citation_message,
            "type": "tool_start"
        }
        self.tool_citations.append(citation)
        logger.info(f"üì§ Tool citation: {tool_name} - {citation_message}")

        # Write citation to state's stream_chunk_data for immediate streaming
        # This triggers LangGraph to emit an event
        if self.state is not None:
            self.state["stream_chunk_data"] = {
                "type": "tool_citation",
                "data": citation
            }

        # Call all registered callbacks for real-time streaming
        callbacks = get_tool_citation_callbacks()
        logger.info(f"üîç DEBUG: Found {len(callbacks)} registered callbacks")
        for callback in callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(citation)
                    logger.info(f"üìÆ Called async callback for tool citation: {tool_name}")
                else:
                    callback(citation)
                    logger.info(f"üìÆ Called sync callback for tool citation: {tool_name}")
            except Exception as e:
                logger.error(f"‚ùå Error calling citation callback: {e}", exc_info=True)

        # Yield control to event loop to allow streaming to happen
        await asyncio.sleep(0)

    def _make_serializable(self, obj: Any) -> Any:
        """
        Recursively convert an object to a JSON-serializable form.
        Filters out non-serializable objects like QueryTracer.

        Args:
            obj: Object to make serializable

        Returns:
            JSON-serializable version of the object
        """
        import json
        from datetime import datetime, date

        # Handle basic JSON-serializable types
        if obj is None or isinstance(obj, (bool, int, float, str)):
            return obj

        # Handle datetime objects - convert to ISO format string
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()

        # Handle dictionaries recursively
        if isinstance(obj, dict):
            result = {}
            for key, value in obj.items():
                try:
                    # Try to serialize the value
                    serialized_value = self._make_serializable(value)
                    # Double-check it's actually JSON-serializable
                    json.dumps(serialized_value)
                    result[key] = serialized_value
                except (TypeError, ValueError):
                    # Skip non-serializable values
                    logger.debug(f"Skipping non-serializable key '{key}' of type {type(value).__name__}")
            return result

        # Handle lists recursively
        if isinstance(obj, (list, tuple)):
            result = []
            for item in obj:
                try:
                    serialized_item = self._make_serializable(item)
                    json.dumps(serialized_item)
                    result.append(serialized_item)
                except (TypeError, ValueError):
                    # Skip non-serializable items
                    logger.debug(f"Skipping non-serializable list item of type {type(item).__name__}")
            return result

        # For other types, try to convert to string representation
        try:
            json.dumps(obj)
            return obj
        except (TypeError, ValueError):
            # Can't serialize this object, skip it
            logger.debug(f"Cannot serialize object of type {type(obj).__name__}")
            return None

    def _build_tool_args_from_state(self, tool_name: str, contract: Dict[str, Any]) -> Dict[str, Any]:
        """
        Pass entire state to tool - tools read what they need themselves.

        Args:
            tool_name: Name of the tool
            contract: Tool contract with "requires" field (for validation/logging)

        Returns:
            Dict with state key
        """
        if contract:
            required_keys = contract.get("requires", [])
            # Log which keys the tool will need
            for key in required_keys:
                if key not in self.state:
                    logger.warning(f"Tool {tool_name} requires '{key}' but not in state")
                else:
                    logger.debug(f"Tool {tool_name} will read '{key}' from state")

        # Create a JSON-serializable copy of state by filtering out non-serializable objects
        serializable_state = self._make_serializable(self.state)

        # Pass entire state to tool
        logger.info(f"Passing state to {tool_name}")
        return {"state": serializable_state}

    def _write_tool_outputs_to_state(self, tool_name: str, result: Dict[str, Any], contract: Dict[str, Any]) -> None:
        """
        Write tool outputs back to state based on contract's "produces" field.

        Args:
            tool_name: Name of the tool
            result: Tool execution result
            contract: Tool contract with "produces" field
        """
        if not contract or not result:
            return

        produced_keys = contract.get("produces", [])

        for key in produced_keys:
            if key in result:
                self.state[key] = result[key]
                logger.info(f"Tool {tool_name} wrote '{key}' to state")
            else:
                logger.debug(f"Tool {tool_name} should produce '{key}' but not in result")

    def _resolve_args(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        Resolve tool arguments from explicit args or args_from references.

        Args:
            tool_call: Tool call definition

        Returns:
            Resolved arguments dict

        Examples:
            {"args": {"query": "nike shoes"}} ‚Üí {"query": "nike shoes"}
            {"args_from": "search.product_search.products"} ‚Üí {products from context}
            {"args_from": "all"} ‚Üí {flattened context}
        """
        tool_name = tool_call.get("name", "unknown")

        if "args" in tool_call:
            # Explicit arguments - but may contain string references that need resolving
            args = tool_call["args"]

            # Check if LLM mistakenly put args_from inside args dict
            if "args_from" in args and len(args) == 1:
                # LLM generated {"args": {"args_from": "path"}} instead of {"args_from": "path"}
                # Treat this as args_from
                path = args["args_from"]
                return self._resolve_args_from(tool_name, path)

            resolved_args = self._resolve_string_references(args)
            logger.debug(f"Tool {tool_name} using explicit args: {list(resolved_args.keys())}")
            return resolved_args

        elif "args_from" in tool_call:
            path = tool_call["args_from"]
            return self._resolve_args_from(tool_name, path)

        else:
            # No args specified
            return {}

    def _resolve_args_from(self, tool_name: str, path: str) -> Dict[str, Any]:
        """
        Resolve arguments from args_from path reference.

        Args:
            tool_name: Name of the tool being called
            path: args_from path (e.g., "step_1.product_search.products", "step_1", "all")

        Returns:
            Resolved arguments dict
        """
        if path == "all":
            # Return flattened context
            args = self._flatten_context()
            logger.debug(f"Tool {tool_name} using args_from='all': {list(args.keys())}")
            return args

        # Parse path like "search.product_search.products"
        parts = path.split(".")
        base_args = {}

        if len(parts) == 1:
            # Reference entire step: "search"
            # Collect all tool results from that step (skip metadata fields)
            step_id = parts[0]
            skip_fields = [
                'success', 'error', 'cached', 'normalized_query', 'timestamp',
                'evidence_citations', 'confidence_score', 'citations', 'link_health',
                'assistant_text', 'ui_blocks', 'destination_overview', 'destination_facts'
            ]
            for key, value in self.context.items():
                if key.startswith(f"{step_id}."):
                    if isinstance(value, dict):
                        for field, field_value in value.items():
                            if field not in skip_fields:
                                base_args[field] = field_value

        elif len(parts) == 2:
            # Reference specific tool output: "search.product_search"
            step_id, tool_name_ref = parts
            key = f"{step_id}.{tool_name_ref}"
            data = self.context.get(key, {})
            # Filter out metadata fields
            skip_fields = ['success', 'error', 'cached', 'normalized_query', 'timestamp']
            base_args = {k: v for k, v in data.items() if k not in skip_fields}

        elif len(parts) >= 3:
            # Reference specific field: "search.product_search.products"
            step_id, tool_name_ref = parts[0], parts[1]
            key = f"{step_id}.{tool_name_ref}"
            data = self.context.get(key, {})

            # Navigate nested path
            for field in parts[2:]:
                if isinstance(data, dict):
                    data = data.get(field, {})
                else:
                    base_args = {}
                    break

            # Wrap in dict if it's a list or value
            if isinstance(data, dict):
                base_args = data
            elif data is not None:
                base_args = {parts[-1]: data}

        # Special case: compose tools need user_message, intent, slots from global state
        if tool_name in ['product_compose', 'travel_compose']:
            if 'user_message' not in base_args and 'user_message' in self.state:
                base_args['user_message'] = self.state['user_message']
                logger.debug(f"Auto-injected user_message for {tool_name}")
            if 'intent' not in base_args and 'intent' in self.state:
                base_args['intent'] = self.state['intent']
                logger.debug(f"Auto-injected intent for {tool_name}")
            if 'slots' not in base_args and 'slots' in self.state:
                base_args['slots'] = self.state['slots']
                logger.debug(f"Auto-injected slots for {tool_name}")

        # Special case: product_ranking needs products from search step
        if tool_name == 'product_ranking' and 'products' not in base_args:
            # Look for products in earlier steps
            for key, value in self.context.items():
                if isinstance(value, dict) and 'products' in value:
                    base_args['products'] = value['products']
                    break

        # Special case: product_normalize needs products from search step
        if tool_name == 'product_normalize' and 'products' not in base_args:
            # Look for products in earlier steps
            for key, value in self.context.items():
                if isinstance(value, dict) and 'products' in value:
                    base_args['products'] = value['products']
                    break

        logger.debug(f"Tool {tool_name} resolved args from '{path}': {list(base_args.keys())}")
        return base_args

    def _resolve_path(self, path: str, param_name: str = None) -> Any:
        """
        Resolve a path reference to actual data from context.

        Args:
            path: Reference path (e.g., "step_1.product_search.products")
            param_name: Optional parameter name for field matching

        Returns:
            Resolved data or None if not found
        """
        parts = path.split(".")

        if len(parts) < 2:
            return None

        step_id, tool_name = parts[0], parts[1]
        context_key = f"{step_id}.{tool_name}"
        data = self.context.get(context_key, {})

        if not data:
            return None

        # If path has field specified (3+ parts), navigate to it
        if len(parts) >= 3:
            for field in parts[2:]:
                if isinstance(data, dict):
                    data = data.get(field)
                else:
                    return None
            return data

        # If no field specified, try to extract based on param_name
        if isinstance(data, dict) and param_name and param_name in data:
            return data[param_name]

        # Otherwise return whole data dict (filtering metadata)
        if isinstance(data, dict):
            skip_fields = ['success', 'error', 'cached', 'normalized_query', 'timestamp']
            return {k: v for k, v in data.items() if k not in skip_fields}

        return data

    def _resolve_string_references(self, args: Dict[str, Any]) -> Dict[str, Any]:
        """
        Resolve string references in explicit args.

        LLM sometimes generates args like:
        {"normalized_products": "step_4.product_normalize.normalized_products"}
        or
        {"products": {"args_from": "step_1.product_search.products"}}

        This should be resolved to the actual value from context.
        """
        resolved = {}
        for key, value in args.items():
            # Check if value is a dict with args_from (nested object reference)
            if isinstance(value, dict) and "args_from" in value:
                # LLM generated {"products": {"args_from": "path"}}
                path = value["args_from"]
                resolved_value = self._resolve_path(path, key)
                if resolved_value is not None:
                    resolved[key] = resolved_value
                    logger.debug(f"Resolved nested args_from '{path}' for key '{key}'")
                else:
                    # Keep original if couldn't resolve
                    resolved[key] = value
            elif isinstance(value, str) and "." in value and not value.startswith("http"):
                # Check if this looks like a reference (has dots and isn't a URL)
                parts = value.split(".")
                if len(parts) >= 2:
                    # Try to resolve as reference
                    step_id, tool_name_ref = parts[0], parts[1]
                    context_key = f"{step_id}.{tool_name_ref}"
                    data = self.context.get(context_key, {})

                    if data and len(parts) >= 3:
                        # Navigate to specific field
                        for field in parts[2:]:
                            if isinstance(data, dict):
                                data = data.get(field)
                            else:
                                break

                        if data is not None:
                            resolved[key] = data
                            logger.debug(f"Resolved reference '{value}' to value")
                        else:
                            # Couldn't resolve - keep original
                            resolved[key] = value
                    elif data:
                        # Reference to entire tool output - use the first data field we find
                        # that matches the parameter name
                        if isinstance(data, dict) and key in data:
                            resolved[key] = data[key]
                            logger.debug(f"Resolved reference '{value}' using field '{key}'")
                        else:
                            resolved[key] = value
                    else:
                        # Not found in context - keep original
                        resolved[key] = value
                else:
                    # Not a reference - keep original
                    resolved[key] = value
            else:
                # Not a string or no dots - keep original
                resolved[key] = value

        return resolved

    def _flatten_context(self) -> Dict[str, Any]:
        """
        Flatten context for tools that need all previous results.

        Returns:
            Flattened dict with all non-error results merged together
        """
        flattened = {}

        # Fields to skip (metadata/internal fields that aren't tool parameters)
        skip_fields = [
            'success', 'error', 'cached', 'normalized_query', 'timestamp',
            'evidence_citations', 'confidence_score', 'citations', 'link_health',
            'assistant_text', 'ui_blocks',  # Output fields from compose tools
            'destination_overview', 'destination_facts',  # Travel metadata fields
            # State fields that shouldn't be passed to tools
            'status', 'current_agent', 'plan', 'halt', 'followup_questions',
            'session_id', 'errors'
        ]

        # First, add relevant global state fields (very selective to avoid polluting tool args)
        global_fields_to_include = ['location']  # Only truly universal fields
        for field in global_fields_to_include:
            if field in self.state and self.state[field] is not None:
                flattened[field] = self.state[field]

        # Then, merge tool results from context
        for key, value in self.context.items():
            if isinstance(value, dict) and not value.get("error"):
                # Merge the value dict into flattened (not nest under tool name)
                for field, field_value in value.items():
                    # Skip internal/metadata fields
                    if field not in skip_fields:
                        # Only set if not already present (tool results win over state fields)
                        if field not in flattened:
                            flattened[field] = field_value

        logger.debug(f"Flattened context keys: {list(flattened.keys())}")
        return flattened

    def _topological_sort(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Sort steps by dependencies using topological sort.

        Args:
            steps: List of step definitions

        Returns:
            List of steps in execution order

        Raises:
            ValueError: If circular dependency detected
        """
        # Build adjacency list and in-degree map
        graph = defaultdict(list)
        in_degree = defaultdict(int)
        step_map = {step["id"]: step for step in steps}

        # Initialize all steps with in-degree 0
        for step in steps:
            step_id = step["id"]
            if step_id not in in_degree:
                in_degree[step_id] = 0

        # Build graph
        for step in steps:
            step_id = step["id"]
            for dep in step.get("depends_on", []):
                graph[dep].append(step_id)
                in_degree[step_id] += 1

        # Kahn's algorithm for topological sort
        queue = [sid for sid in step_map.keys() if in_degree[sid] == 0]
        sorted_ids = []

        while queue:
            # Sort queue for deterministic ordering
            queue.sort()
            current = queue.pop(0)
            sorted_ids.append(current)

            # Reduce in-degree for neighbors
            for neighbor in graph[current]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        # Check for circular dependencies
        if len(sorted_ids) != len(steps):
            remaining = set(step_map.keys()) - set(sorted_ids)
            raise ValueError(f"Circular dependency detected involving steps: {remaining}")

        # Return steps in sorted order
        return [step_map[sid] for sid in sorted_ids]

    def _is_critical_step(self, step: Dict[str, Any]) -> bool:
        """
        Determine if a step is critical (failure should abort execution).

        Args:
            step: Step definition

        Returns:
            True if step is critical
        """
        # Compose steps are usually critical (final output)
        tool_names = step.get("tools", [])  # Now just a list of strings

        critical_tools = {"product_compose", "travel_compose", "general_compose"}

        return any(tool in critical_tools for tool in tool_names)

    def _extract_results(self) -> Dict[str, Any]:
        """
        Extract final results from execution context.

        Returns:
            Dict with assistant_text, ui_blocks, citations, etc.
        """
        results = {}

        # Look for compose tool results (final output)
        logger.debug(f"üîç Extracting results from context keys: {list(self.context.keys())}")
        for key, value in self.context.items():
            if "compose" in key and isinstance(value, dict):
                # This is likely the final composed response
                logger.info(f"‚úì Found compose tool result in key '{key}'")
                logger.debug(f"  - assistant_text: {len(value.get('assistant_text', ''))} chars")
                logger.debug(f"  - ui_blocks: {len(value.get('ui_blocks', []))} blocks")
                logger.debug(f"  - citations: {len(value.get('citations', []))} citations")
                results["assistant_text"] = value.get("assistant_text", "")
                results["ui_blocks"] = value.get("ui_blocks", [])
                results["citations"] = value.get("citations", [])

        # Look for next_step_suggestion results
        for key, value in self.context.items():
            if "next_step_suggestion" in key and isinstance(value, dict):
                results["next_suggestions"] = value.get("next_suggestions", [])

        # If no compose result found, check for other tool outputs
        if not results.get("assistant_text"):
            # Extract raw tool outputs
            results["tool_outputs"] = self.context.copy()

        # Add execution metadata
        results["execution_context"] = {
            "steps_executed": len([k for k in self.context.keys() if "." in k]),
            "errors": [k for k, v in self.context.items() if isinstance(v, dict) and v.get("error")]
        }

        # Include tool citations for streaming
        if self.tool_citations:
            results["tool_citations"] = self.tool_citations
            logger.info(f"üìã Collected {len(self.tool_citations)} tool citations")

        logger.info(f"üì¶ Extracted results: assistant_text={bool(results.get('assistant_text'))}, ui_blocks={len(results.get('ui_blocks', []))}, citations={len(results.get('citations', []))}")

        return results
